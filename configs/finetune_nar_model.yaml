seed_everything: 42
trainer:
  #resume_from_checkpoint: "/root/autodl-tmp/models/lyh-nar/epoch_32_1800.ckpt"
  logger: true
  default_root_dir: "/root/autodl-tmp/models/lyh-nar"
  accelerator: gpu
  devices: 1
  strategy: ddp_find_unused_parameters_false
  accumulate_grad_batches: 2
  precision: 16
  val_check_interval: 100
  max_steps: 300000
  gradient_clip_val: 0.5
  callbacks:
    - class_path: pytorch_lightning.callbacks.ModelCheckpoint
      init_args:
        save_top_k: 3 # save k best models (determined by above metric)
        monitor: 'val/loss'
        mode: 'min'
        save_last: True # additionaly always save model from last epoch
        verbose: True
        dirpath: "/root/autodl-tmp/models/lyh-nar"
        filename: "epoch_{epoch}_{step}"
        auto_insert_metric_name: False
        every_n_train_steps: 100
    - class_path: pytorch_lightning.callbacks.LearningRateMonitor
      init_args:
        logging_interval: step

model:
  class_path: vc_lm.models.nar_model_pl.NARModelPL
  init_args:
    config_file: configs/nar_model.json
    lr: 0.00001
    weight_decay: 0.01
    warmup_step: 100
    max_iters: 300000

data:
  class_path: vc_lm.datamodules.nar_datamodule.NARDataModule
  init_args:
    data_dir: "/root/autodl-tmp/data/lyh-p-wds"
    batch_size: 6
    max_audio_time: 24
    num_workers: 1
    # 2262692
    train_dataset_size: 2180
    train_pattern: "shard-{000000..000000}.tar"
    val_dataset_size: 8
    val_pattern: "shard-{000000..000000}.tar"
